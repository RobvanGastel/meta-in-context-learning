{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for Google Collab\n",
    "\n",
    "# Set the runtime session to a GPU/TPU session first!\n",
    "# Clone the repository\n",
    "!git clone https://github.com/RobvanGastel/meta-in-context-learning.git\n",
    "\n",
    "# Change directory to the cloned repository\n",
    "%cd meta-in-context-learning\n",
    "\n",
    "# Potentially this is the only dependency not supported yet\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "from flax import serialization\n",
    "from flax.training import train_state\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "\n",
    "from meta_icl.vision_transformer import ViT\n",
    "from meta_icl.data import FewShotDataset, FewShotBatchSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Outline of the General Purpose In-Context Learning (GPICL) Method\n",
    "\n",
    "The GPICL method was the paper that introduced the idea to use the capability of in-context learning more generally. It casts a single few-shot learning task of n-shot or n-samples with k-way for k classes as a \"single\" sample. The model processes input pairs $X_i, y_i$ while leaving out the label for the last sample. The task is to predict this label as you would do in classification setting, but now with the context of a small few-shot task.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"assets/gpicl.png\" alt=\"General Purpose In-Context Learning\" width=\"70%\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "To increase the number of possible tasks in MNIST, or FashionMNIST the dataset $D = \\{x_i, y_i\\}$ is augmented by a linear projection $A \\in \\mathcal{R}^{N_x \\times N_x}$ with $A_{ij} \\sim N(0, 1/N_x)$ and an permutation on the labels $\\rho$ , $D = \\{Ax_i, \\rho(y_i)\\}$. Finally these tasks are used to meta-learn our model, such that for any few-shot task it can infer from the task what the label of our query sample would be. Even to some extend out-of-distribution tasks.\n",
    "\n",
    "Compared to the other ``in-context-learning mechanism.ipynb`` notebook, it seems like increasing the complexity from regression to image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "epochs = 100\n",
    "n_way, k_shot = 3, 2\n",
    "seq_length = n_way * k_shot\n",
    "batch_size = 512\n",
    "seed = 42\n",
    "\n",
    "vit_model = ViT(\n",
    "    image_size = 28,\n",
    "    patch_size = (14, 14),\n",
    "    num_classes = 10,\n",
    "    emb_dim = 256,\n",
    "    seq_length = seq_length,\n",
    "    channels = 1,\n",
    "    num_layers = 4,\n",
    "    num_heads = 8,\n",
    "    mlp_dim = 512\n",
    ")\n",
    "\n",
    "key = jax.random.key(seed)\n",
    "key, x_key, y_key, rng_key = jax.random.split(key, 4)\n",
    "\n",
    "X = jax.random.normal(x_key, (batch_size, seq_length, 28*28))\n",
    "y = jax.random.normal(y_key, (batch_size, seq_length-1))\n",
    "params = vit_model.init({'params': rng_key}, X, y)\n",
    "output = vit_model.apply(params, X, y, rngs={'params': rng_key})\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    pass\n",
    "\n",
    "state = TrainState.create(\n",
    "    apply_fn=vit_model.apply,\n",
    "    params=params,\n",
    "    tx=optax.adamw(learning_rate=1e-4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you desire to take trained weights of epoch 600, run this cell\n",
    "with open(\"output/gpicl_e600.pkl\", \"rb\") as f:\n",
    "    state_dict = pickle.loads(f.read())\n",
    "params = serialization.from_state_dict(params, state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the functions for the training loop, like GPICL we will augment the few-shot learning tasks in each batch with a linear project $A$ and a permutation function for the classes $\\rho(y)$. Here p is the probability of permutating the entire task or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def augment_tasks(X, y, key, p=0.0):\n",
    "    batch_size, seq, _, _ = X.shape\n",
    "    key_A, key_perm, key_apply = jax.random.split(key, 3)\n",
    "\n",
    "    # Linear projection A, A_ij \\in N(0, 1/Nx)\n",
    "    X_bar = jnp.reshape(X, (batch_size, seq, 28*28))\n",
    "    A = jax.random.normal(key_A, (batch_size, 28*28), dtype=jnp.float32) / 28\n",
    "    # Shape: (batch_size, seq_length, 28*28)\n",
    "    X_bar = jnp.einsum('bsd,bd->bsd', X_bar, A)\n",
    "\n",
    "    # Should have permutation \\rho(y), to create a new mapping\n",
    "    perm  = jax.random.permutation(key_perm, 10) # num_classes\n",
    "    apply = jax.random.bernoulli(key_apply, p)\n",
    "    y_bar = jnp.where(apply, perm[y], y)\n",
    "\n",
    "    return X_bar, y_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working in JAX we can speed-up our method with @jax.jit when applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(state, X, y, train_key, eval_key):\n",
    "    def loss_fn(params, X, y):\n",
    "        logits = state.apply_fn(params, X, y[:, :-1])\n",
    "        # Compare against the last y label in the few-shot task and omit this y label during\n",
    "        # the forward pass.\n",
    "        y_one_hot = jax.nn.one_hot(y, 10)[:, -1].squeeze()\n",
    "        loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=y_one_hot))\n",
    "        return loss\n",
    "\n",
    "    # Task augmentation\n",
    "    X_bar, y_bar = augment_tasks(X, y, train_key) # Optional add permutations\n",
    "\n",
    "    loss_grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = loss_grad_fn(state.params, X_bar, y_bar)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "\n",
    "    # Validating the performance with a different evaluation key\n",
    "    X_bar, y_bar = augment_tasks(X, y, eval_key)\n",
    "    logits = state.apply_fn(state.params, X_bar, y[:, :-1])\n",
    "    y_one_hot = jax.nn.one_hot(y, 10)[:, -1].squeeze()\n",
    "    acc = jnp.sum(y[:, -1] == jnp.argmax(logits, axis=-1))\n",
    "    return state, loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop for our GPICL model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FewShotDataset(dataset=MNIST, train=True)\n",
    "data_loader = DataLoader(train_dataset, batch_sampler=FewShotBatchSampler(\n",
    "    train_dataset.y, n_way, k_shot, batch_size=batch_size\n",
    "    )\n",
    ")\n",
    "\n",
    "# Meta-training loop\n",
    "metrics = {\n",
    "    \"loss\": [],\n",
    "    \"accuracy\": []\n",
    "}\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    accuracy = 0.0\n",
    "    \n",
    "    for X, y in data_loader:\n",
    "        key, train_key, eval_key = jax.random.split(key, 3)\n",
    "        state, loss, acc = train_step(state, X.numpy(), y.numpy(), train_key, eval_key)\n",
    "        train_loss += loss.mean()\n",
    "        accuracy += (acc / config.batch_size)\n",
    "        \n",
    "    metrics[\"loss\"].append(float(train_loss))\n",
    "    metrics[\"accuracy\"].append(float(accuracy))\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"epoch {epoch}/{epochs}: loss: {train_loss}, accuracy: {accuracy}\")\n",
    "\n",
    "        # Save the weights\n",
    "        with open(f\"output/gpicl_e{epoch}.pkl\", \"wb\") as f:\n",
    "            f.write(pickle.dumps(serialization.to_state_dict(params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the training loss and accuracy during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(len(metrics[\"loss\"])), metrics[\"loss\"], label='Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(len(metrics[\"accuracy\"])), metrics[\"accuracy\"], label='Evaluation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Evaluation Accuracy over Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comment on instance memorization, task memorization,\n",
    "# and learning to learn include the plot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"assets/learning_behavior.png\" alt=\"Meta-learned behavior\" width=\"70%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect the performance of our model we inspect a single batch from the validation set to see if it is able to infer the correct labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = FewShotDataset(dataset=MNIST, train=False)\n",
    "val_data_loader = DataLoader(val_dataset, batch_sampler=FewShotBatchSampler(val_dataset.y, n_way, k_shot))\n",
    "\n",
    "X, y = next(iter(val_data_loader))\n",
    "\n",
    "X = X.numpy()\n",
    "y = y.numpy()\n",
    "\n",
    "batch_size, seq, _, _ = X.shape\n",
    "logits = state.apply_fn(state.params, X, y[:, :-1])\n",
    "jnp.sum(y[:, -1] == jnp.argmax(logits, axis=-1))\n",
    "jnp.argmax(logits, axis=-1), y[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Show task visualization and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Show the performance on the fashionMNIST task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_dataset = FewShotDataset(dataset=FashionMNIST, train=True)\n",
    "fashion_data_loader = DataLoader(fashion_dataset, batch_sampler=FewShotBatchSampler(\n",
    "    train_dataset.y, n_way, k_shot, batch_size=batch_size\n",
    "    )\n",
    ")\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Show difference with label permutation model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
