{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for Google Collab\n",
    "\n",
    "# Set the runtime session to a GPU/TPU session first!\n",
    "# Clone the repository\n",
    "!git clone https://github.com/RobvanGastel/meta-in-context-learning.git\n",
    "\n",
    "# Change directory to the cloned repository\n",
    "%cd meta-in-context-learning\n",
    "\n",
    "# Potentially this is the only dependency not supported yet\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Outline of GPICL\n",
    "Dataset $D = \\{x_i, y_i\\}$, linear projection $A \\in \\mathcal{R}^{N_x \\times N_x}$ with $A_{ij} \\sim N(0, 1/N_x)$ and final output permutation $\\rho$ , $D = \\{Ax_i, \\rho(y_i)\\}$.\n",
    "\n",
    "this is done to reduce the amount of unique tasks necessary to train our meta-learned model. The loss used is cross-entropy loss, between the label y_j and prediction on the entire series except for the last label. Essentially the same the other notebook leaving out only the last label and adding the set of samples as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "from flax.training import train_state\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from meta_icl.vision_transformer import ViT\n",
    "from meta_icl.data import FewShotDataset, FewShotBatchSampler\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 40000\n",
    "n_way, k_shot = 3, 2\n",
    "batch_size = 8\n",
    "seq_length = 6 \n",
    "seed = 42\n",
    "\n",
    "v = ViT(\n",
    "    image_size = 28,\n",
    "    patch_size = (14, 14),\n",
    "    num_classes = 10,\n",
    "    emb_dim = 256,\n",
    "    seq_length = seq_length,\n",
    "    channels = 1,\n",
    "    num_layers = 4,\n",
    "    num_heads = 8,\n",
    "    mlp_dim = 512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key = jax.random.key(seed)\n",
    "init_rngs = {'params': jax.random.key(1)}\n",
    "\n",
    "X = jax.random.normal(key, (batch_size, seq_length, 28*28))\n",
    "y = jax.random.normal(key, (batch_size, seq_length-1))\n",
    "\n",
    "params = v.init(init_rngs, X, y)\n",
    "output = v.apply(params, X, y, rngs=init_rngs)\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    pass\n",
    "\n",
    "state = TrainState.create(\n",
    "    apply_fn=v.apply,\n",
    "    params=params,\n",
    "    tx=optax.adamw(learning_rate=1e-4)\n",
    ")\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, X, y):\n",
    "    def loss_fn(params):\n",
    "        batch_size, seq, _, _ = X.shape\n",
    "\n",
    "        # Linear projection A, A_ij \\in N(0, 1/Nx)\n",
    "        # X_bar = X\n",
    "        X_bar = jnp.reshape(X, (batch_size, seq, 28*28))\n",
    "        A = (jax.random.normal(key, (batch_size, 28*28), dtype=jnp.float32) * jnp.array(1/28, dtype=jnp.float32))\n",
    "        X_bar = jnp.einsum(\"bsj,bj->bsj\", X_bar, A)\n",
    "\n",
    "        # TODO: Should have permutation \\rho(y)\n",
    "\n",
    "        logits = state.apply_fn(params, X_bar, y[:, :-1])\n",
    "\n",
    "        logits = jnp.expand_dims(logits, axis=0)\n",
    "        y_hat = jnp.argmax(jax.nn.softmax(logits))\n",
    "        y_one_hot = jax.nn.one_hot(y, 10)[:, 8:9]\n",
    "        loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=y_one_hot))\n",
    "        return loss\n",
    "\n",
    "    loss_grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = loss_grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "train_dataset = FewShotDataset(dataset=MNIST, train=True)\n",
    "data_loader = DataLoader(train_dataset, batch_sampler=FewShotBatchSampler(\n",
    "    train_dataset.y, n_way, k_shot, batch_size=batch_size\n",
    "    )\n",
    ")\n",
    "\n",
    "# Meta-training loop\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    for X, y in data_loader:\n",
    "        state, loss = train_step(state, X.numpy(), y.numpy())\n",
    "        cumulative_loss += loss.mean()\n",
    "\n",
    "    losses.append(float(cumulative_loss))\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"epoch {epoch}/{num_epochs}: loss: {cumulative_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(num_epochs), losses, label='Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
